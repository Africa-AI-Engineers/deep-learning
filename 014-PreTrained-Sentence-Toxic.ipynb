{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Vectors - Sentence -  Toxic Comments\n",
    "\n",
    "A corpus of manually labeled comments - classifying each comment by its type of toxicity is available on Kaggle. We will aim to do a binary classification of whether a comment is toxic or not\n",
    "\n",
    "This notebook uses **Pre-trained Sentence Embedding** from Spacy to do the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment these shell lines to get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://bit.do/deep_toxic_train -P data/\n",
    "#!mv data/deep_toxic_train data/train.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Input & Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Explanation\\nWhy the edits made under my usern...\n",
       "1    D'aww! He matches this background colour I'm s...\n",
       "2    Hey man, I'm really not trying to edit war. It...\n",
       "3    \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4    You, sir, are my hero. Any chance you remember...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = df[\"comment_text\"]\n",
    "train_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing the train data**\n",
    "- Tokenization: \"This is an apple\" -> [\"This\", \"is\", \"an\", \"apple\"]\n",
    "- Indexing: {0: \"This\", 1: \"is\", 2: \"an\", 3: \"apple\"}\n",
    "- Index Representation: [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "max_features = 2000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Representation\n",
    "tokenized_train = tokenizer.texts_to_sequences(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Selecting Padding\n",
    "# find length of each sentence and plot the length\n",
    "number_of_words = [len(comment) for comment in tokenized_train]\n",
    "plt.hist(number_of_words, bins = np.arange(0, 500, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding to make it uniform\n",
    "maxlen = 200\n",
    "X = pad_sequences(tokenized_train, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.iloc[:,2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90415551697990237"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline Benchmark \n",
    "1 - df.iloc[:,2].sum()/df.iloc[:,2].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((127656, 200), (31915, 200), (127656, 2), (31915, 2))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1a: Creating pre-trained Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is some text that I am processing with Spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[3].vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence vector**\n",
    "\n",
    "Spacy also provides an option of sentence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(text):\n",
    "    doc = nlp(text)\n",
    "    return doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm.pandas(desc=\"progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this didn't run in reasonable time\n",
    "# df2[\"word_embedding\"] = df2[\"comment_text\"].apply(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 7.33 s, total: 1min 14s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedding_matrix = df2.iloc[:1000,1].apply(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.0147541, 0.0950991, -0.182347, -0.0324759, ...\n",
       "1    [-0.0541294, 0.254045, -0.045151, -0.0559909, ...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the embedding matrix\n",
    "emb_mat = np.empty([1000, 300])\n",
    "j = 0\n",
    "for i in embedding_matrix:\n",
    "    emb_mat[j] = i\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 300)          300000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 60000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1920032   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 2,220,098\n",
      "Trainable params: 2,220,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(1000, 300, input_length=maxlen, t))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([emb_mat])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Compile the Model & Fit on the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.fit(X_train[:1000], y_train, batch_size=128, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 31915 samples\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1.3040 - acc: 0.8470 - val_loss: 1.4149 - val_acc: 0.9042\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1.2335 - acc: 0.8635 - val_loss: 1.4839 - val_acc: 0.9042\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train[:1000], y_train[:1000],\n",
    "                    epochs=2,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.fit(X_train, y_train, batch_size=128, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.metrics(output.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Visualise evaluation & Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_classes = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test, predict_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"temperature is quite hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
